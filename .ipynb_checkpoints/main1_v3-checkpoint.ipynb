{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea607c8-a216-439f-8e53-401b0f9bb47c",
   "metadata": {},
   "source": [
    "Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664f6037-b077-4d94-9640-edb81aafa0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from data_utils import load\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20625c56-549e-4832-8d77-d57b156d9146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 31.43 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Total RAM in GB\n",
    "total_ram = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"Total RAM: {total_ram:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae86113c-964e-4029-8989-64fffa93d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de701fdc-68d7-4ad8-a3c0-801f3d1eb4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Salesforce/blip-image-captioning-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Salesforce/blip-image-captioning-base loaded successfully.\n",
      "\n",
      "Testing Salesforce/blip2-flan-t5-xl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type blip-2 to instantiate a model of type blip. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29b4c3bc33246aa82fab12f4f72369f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Salesforce/blip2-flan-t5-xl failed: Error(s) in loading state_dict for BlipVisionEmbeddings:\n",
      "\tsize mismatch for class_embedding: copying a param with shape torch.Size([1, 1, 1408]) from checkpoint, the shape in current model is torch.Size([1, 1, 768]).\n",
      "\n",
      "Testing Salesforce/blip2-opt-2.7b...\n",
      " Salesforce/blip2-opt-2.7b failed: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "models = [\n",
    "    \"Salesforce/blip-image-captioning-base\",\n",
    "    \"Salesforce/blip2-flan-t5-xl\",\n",
    "    \"Salesforce/blip2-opt-2.7b\"\n",
    "]\n",
    "\n",
    "for name in models:\n",
    "    try:\n",
    "        print(f\"Testing {name}...\")\n",
    "        processor = BlipProcessor.from_pretrained(name, local_files_only=True)\n",
    "        model = BlipForConditionalGeneration.from_pretrained(name, local_files_only=True)\n",
    "        print(f\" {name} loaded successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\" {name} failed: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f231f81-3ab7-4aa1-a91b-c2cba3425d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa634f61e18e4cec9132365053f1ca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: cigarette lighters\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 1️⃣ Load processor and model\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "\n",
    "# Select device (CPU in this case)\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load an image\n",
    "img_path = \"data/Images_test-20240424T090736Z-001/Images_test/1.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = \"Identify tobacco brands in this picture.\"\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "# Decode and print caption\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated caption:\", caption)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87841f1a-18a6-4679-b5a8-1254c384cf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ec0eda6b174626b754ea8fd10a1ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: cigarettes\n"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load BLIP-2 small (faster, less memory)\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load an image\n",
    "img_path = \"data/Images_test-20240424T090736Z-001/Images_test/2.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Generate caption\n",
    "prompt = \"Describe any tobacco products in this image.\"\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6c6cd1-7d19-421a-9537-c121eb616fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "C:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85fde3bf5fd4ae2a6770c8e1a61fe16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Charger le modèle et le processeur\n",
    "model_name = \"Qwen/Qwen2-VL-7B-Instruct\"  # ou \"Qwen/Qwen2-VL-2B-Instruct\" si tu veux un plus petit modèle\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_name)\n",
    "print(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d8d04-22dc-4f3c-9b3d-c635c97de454",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data/Images_test-20240424T090736Z-001/Images_test/1.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "print(\"-\")\n",
    "# Generate caption\n",
    "prompt = \"Describe any tobacco products in this image.\"\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "print(\"-\")\n",
    "# Générer la réponse\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15680d06-769c-4bb8-85a3-451e69d85f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected car with confidence 0.835 at location [1.32, 1118.76, 1226.91, 1830.19]\n",
      "Detected person with confidence 0.822 at location [1186.63, 932.25, 1388.06, 1438.77]\n",
      "Detected person with confidence 0.815 at location [592.0, 786.17, 738.98, 1182.0]\n",
      "Detected car with confidence 0.586 at location [2097.77, 1041.53, 2766.14, 1831.14]\n",
      "Detected bench with confidence 0.532 at location [1876.75, 950.63, 2419.83, 1186.25]\n",
      "Detected backpack with confidence 0.503 at location [625.07, 851.05, 707.43, 983.36]\n",
      "Detected person with confidence 0.443 at location [2211.33, 874.89, 2353.19, 1070.74]\n",
      "Detected person with confidence 0.363 at location [94.89, 991.79, 137.52, 1125.23]\n",
      "Detected person with confidence 0.239 at location [2224.38, 875.44, 2352.0, 955.89]\n",
      "Detected backpack with confidence 0.216 at location [1233.98, 1016.32, 1352.48, 1153.64]\n",
      "Detected tv with confidence 0.203 at location [1110.09, 661.41, 1727.29, 931.18]\n",
      "Detected bench with confidence 0.202 at location [1885.31, 952.4, 2414.69, 1092.85]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, DetaForObjectDetection\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "img_path = \"data/Images_test-20240424T090736Z-001/Images_test/2.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"jozhang97/deta-swin-large\")\n",
    "model = DetaForObjectDetection.from_pretrained(\"jozhang97/deta-swin-large\")\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = image_processor.post_process_object_detection(outputs, threshold=0.2, target_sizes=target_sizes)[\n",
    "    0\n",
    "]\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48404f7a-8035-4c10-80ef-bfbaa7fe9aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects:\n",
      "Label: cigar, Score: 0.607\n",
      "Label: smoke, Score: 0.505\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the zero-shot object detection pipeline with the specified model\n",
    "detector = pipeline(\"zero-shot-object-detection\", model=\"iSEE-Laboratory/llmdet_large\")\n",
    "\n",
    "# Define the candidate classes (tobacco-related items)\n",
    "candidate_classes = [\"cigarette\", \"vape\", \"lighter\", \"cigar\", \"tobacco pouch\",\"smoke\"]\n",
    "\n",
    "# Load an example image\n",
    "img_path = \"data/Images_test-20240424T090736Z-001/Images_test/2.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Perform zero-shot object detection\n",
    "results = detector(image, candidate_labels=candidate_classes, threshold=0.5)\n",
    "\n",
    "print(\"Detected objects:\")\n",
    "for result in results:\n",
    "    print(f\"Label: {result['label']}, Score: {result['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c233a094-6d1c-46c3-85fe-d485fda87ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected objects:\n",
      "[{'score': 0.6067025661468506, 'label': 'cigar', 'box': {'xmin': 1287, 'ymin': 976, 'xmax': 1332, 'ymax': 1010}}, {'score': 0.5047281980514526, 'label': 'smoke', 'box': {'xmin': 1288, 'ymin': 884, 'xmax': 1659, 'ymax': 1019}}]\n",
      "Label: cigar, Score: 0.607\n",
      "Label: smoke, Score: 0.505\n",
      "(2767, 1847)\n"
     ]
    }
   ],
   "source": [
    "print(\"Detected objects:\")\n",
    "print(results)\n",
    "for result in results:\n",
    "    print(f\"Label: {result['label']}, Score: {result['score']:.3f}\")\n",
    "print(image.size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcba59cb-7a6a-4176-bead-f55eedba8e01",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for 'Salesforce/Salesforce/blip2-flan-t5-small'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Salesforce/Salesforce/blip2-flan-t5-small' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 479\u001b[0m     hf_hub_download(\n\u001b[0;32m    480\u001b[0m         path_or_repo_id,\n\u001b[0;32m    481\u001b[0m         filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    482\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    483\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    484\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    485\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    486\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    487\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    488\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    489\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    490\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    491\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'Salesforce/Salesforce/blip2-flan-t5-small'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_base.py:337\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     resolved_image_processor_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    334\u001b[0m         resolved_file\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [image_processor_file, PROCESSOR_NAME]\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 337\u001b[0m             resolved_file \u001b[38;5;241m:=\u001b[39m cached_file(\n\u001b[0;32m    338\u001b[0m                 pretrained_model_name_or_path,\n\u001b[0;32m    339\u001b[0m                 filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    340\u001b[0m                 cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    341\u001b[0m                 force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    342\u001b[0m                 proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    343\u001b[0m                 resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    344\u001b[0m                 local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    345\u001b[0m                 token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    346\u001b[0m                 user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    347\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    348\u001b[0m                 subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    349\u001b[0m                 _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    350\u001b[0m             )\n\u001b[0;32m    351\u001b[0m         )\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     ]\n\u001b[0;32m    354\u001b[0m     resolved_image_processor_file \u001b[38;5;241m=\u001b[39m resolved_image_processor_files[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mTries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    323\u001b[0m file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:532\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[0;32m    531\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 532\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[0;32m    534\u001b[0m ]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:143\u001b[0m, in \u001b[0;36m_get_cache_file_to_return\u001b[1;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_cache_file_to_return\u001b[39m(\n\u001b[0;32m    136\u001b[0m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    137\u001b[0m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m ):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m try_to_load_from_cache(\n\u001b[0;32m    144\u001b[0m         path_or_repo_id, full_filename, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, revision\u001b[38;5;241m=\u001b[39mrevision, repo_type\u001b[38;5;241m=\u001b[39mrepo_type\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file \u001b[38;5;241m!=\u001b[39m _CACHED_NO_EXIST:\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'Salesforce/Salesforce/blip2-flan-t5-small'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m processor \u001b[38;5;241m=\u001b[39m Blip2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/Salesforce/blip2-flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Blip2ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/Salesforce/blip2-flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# force CPU\u001b[39;00m\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\processing_utils.py:1394\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m-> 1394\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1395\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\processing_utils.py:1453\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1451\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_possibly_dynamic_module(class_name)\n\u001b[1;32m-> 1453\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(attribute_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_base.py:201\u001b[0m, in \u001b[0;36mImageProcessingMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m--> 201\u001b[0m image_processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_image_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(image_processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\TEMP\\anaconda3\\Lib\\site-packages\\transformers\\image_processing_base.py:361\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    362\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load image processor for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m it from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_processor_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m         )\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load image processor for 'Salesforce/Salesforce/blip2-flan-t5-small'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Salesforce/Salesforce/blip2-flan-t5-small' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "device = \"cpu\"  # force CPU\n",
    "model.to(device)\n",
    "\n",
    "image = Image.open(\"data/Images_test-20240424T090736Z-001/Images_test/1.jpg\").convert(\"RGB\")\n",
    "prompt = \"Question: Describe any tobacco products in this image.\"\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)  # limit tokens\n",
    "\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76bd7ba7-cb52-4f5d-b4a4-05f2882c6adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: a cup of coffee and a box of cigarettes\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load processor and model (image-captioning base)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load image\n",
    "img_path = \"data/Images_test-20240424T090736Z-001/Images_test/1.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode and print\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated caption:\", caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
